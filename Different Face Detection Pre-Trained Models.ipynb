{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dlib Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dlib is an open source framework used to figure out facial landmarks which are the salient features of faces like eyes ,  mouth and eyebrows. But here we going to use this to identy or localize face in an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Faces ==> 1\n",
      "CPU times: user 114 ms, sys: 0 ns, total: 114 ms\n",
      "Wall time: 113 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "img = cv2.imread(\"images/single_face.jpg\")\n",
    "bbox = face_detector(img)\n",
    "print(\"Number of Faces ==>\",len(bbox))\n",
    "\n",
    "for faces in bbox:\n",
    "    x,y,width,height = faces.left(),faces.top(),faces.right() - faces.left(),faces.bottom()-faces.top()\n",
    "    cv2.rectangle(img,(x,y),(x+width,y+height),(255,0,0),2)\n",
    "\n",
    "cv2.imwrite(\"output_images/dlib_single_face.jpg\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Faces ==> 11\n",
      "CPU times: user 384 ms, sys: 12 ms, total: 396 ms\n",
      "Wall time: 401 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "img = cv2.imread(\"images/multiple_faces.jpg\")\n",
    "bbox = face_detector(img)\n",
    "\n",
    "print(\"Number of Faces ==>\",len(bbox))\n",
    "\n",
    "for faces in bbox:\n",
    "    x,y,width,height = faces.left(),faces.top(),faces.right() - faces.left(),faces.bottom()-faces.top()\n",
    "    cv2.rectangle(img,(x,y),(x+width,y+height),(255,0,0),2)\n",
    "    \n",
    "cv2.imwrite(\"output_images/dlib_multiple_faces.jpg\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dlib.rectangles"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Faces ==> 39\n",
      "CPU times: user 1.94 s, sys: 56.7 ms, total: 1.99 s\n",
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "img = cv2.imread(\"images/group_faces.jpg\")\n",
    "bbox = face_detector(img)\n",
    "\n",
    "print(\"Number of Faces ==>\",len(bbox))\n",
    "\n",
    "for faces in bbox:\n",
    "    x,y,width,height = faces.left(),faces.top(),faces.right() - faces.left(),faces.bottom()-faces.top()\n",
    "    cv2.rectangle(img,(x,y),(x+width,y+height),(255,0,0),5)\n",
    "    \n",
    "cv2.imwrite(\"output_images/dlib_group_faces.jpg\",img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/whirldata/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "face_detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Faces ==> 1\n",
      "CPU times: user 708 ms, sys: 53.4 ms, total: 762 ms\n",
      "Wall time: 461 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "img = cv2.cvtColor(cv2.imread(\"images/single_face.jpg\"), cv2.COLOR_BGR2RGB)\n",
    "face_info = face_detector.detect_faces(img)\n",
    "\n",
    "print(\"Number of Faces ==>\",len(face_info))\n",
    "\n",
    "for faces in face_info:\n",
    "    x,y,width,height = faces[\"box\"]\n",
    "    cv2.rectangle(img,(x,y),(x+width,y+height),(255,0,0),2)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "cv2.imwrite(\"output_images/mtcnn_single_face.jpg\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'box': [208, 122, 155, 207],\n",
       "  'confidence': 1.0,\n",
       "  'keypoints': {'left_eye': (243, 205),\n",
       "   'mouth_left': (249, 276),\n",
       "   'mouth_right': (311, 274),\n",
       "   'nose': (271, 244),\n",
       "   'right_eye': (306, 203)}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Faces ==> 12\n",
      "CPU times: user 4.06 s, sys: 308 ms, total: 4.37 s\n",
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "img = cv2.cvtColor(cv2.imread(\"images/multiple_faces.jpg\"), cv2.COLOR_BGR2RGB)\n",
    "face_info = face_detector.detect_faces(img)\n",
    "\n",
    "print(\"Number of Faces ==>\",len(face_info))\n",
    "\n",
    "for faces in face_info:\n",
    "    x,y,width,height = faces[\"box\"]\n",
    "    cv2.rectangle(img,(x,y),(x+width,y+height),(255,0,0),2)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "cv2.imwrite(\"output_images/mtcnn_multiple_face.jpg\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Faces ==> 42\n",
      "CPU times: user 20.4 s, sys: 2.58 s, total: 23 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "img = cv2.cvtColor(cv2.imread(\"images/group_faces.jpg\"), cv2.COLOR_BGR2RGB)\n",
    "face_info = face_detector.detect_faces(img)\n",
    "\n",
    "print(\"Number of Faces ==>\",len(face_info))\n",
    "\n",
    "for faces in face_info:\n",
    "    x,y,width,height = faces[\"box\"]\n",
    "    cv2.rectangle(img,(x,y),(x+width,y+height),(255,0,0),5)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "cv2.imwrite(\"output_images/mtcnn_group_face.jpg\",img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Face Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tiny_face_model\n",
    "import util\n",
    "import cv2\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle\n",
    "import pylab as pl\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from scipy.special import expit\n",
    "import glob\n",
    "\n",
    "MAX_INPUT_DIM = 5000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_scales():\n",
    "        raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n",
    "        min_scale = min(np.floor(np.log2(np.max(clusters_w[normal_idx] / raw_w))),\n",
    "                        np.floor(np.log2(np.max(clusters_h[normal_idx] / raw_h))))\n",
    "        max_scale = min(1.0, -np.log2(max(raw_h, raw_w) / MAX_INPUT_DIM))\n",
    "        scales_down = pl.frange(min_scale, 0, 1.)\n",
    "        scales_up = pl.frange(0.5, max_scale, 0.5)\n",
    "        scales_pow = np.hstack((scales_down, scales_up))\n",
    "        scales = np.power(2.0, scales_pow)\n",
    "        return scales\n",
    "    \n",
    "def _calc_bounding_boxes():\n",
    "    # threshold for detection\n",
    "    _, fy, fx, fc = np.where(prob_cls_tf > prob_thresh)\n",
    "\n",
    "    # interpret heatmap into bounding boxes\n",
    "    cy = fy * 8 - 1\n",
    "    cx = fx * 8 - 1\n",
    "    ch = clusters[fc, 3] - clusters[fc, 1] + 1\n",
    "    cw = clusters[fc, 2] - clusters[fc, 0] + 1\n",
    "\n",
    "    # extract bounding box refinement\n",
    "    Nt = clusters.shape[0]\n",
    "    tx = score_reg_tf[0, :, :, 0:Nt]\n",
    "    ty = score_reg_tf[0, :, :, Nt:2*Nt]\n",
    "    tw = score_reg_tf[0, :, :, 2*Nt:3*Nt]\n",
    "    th = score_reg_tf[0, :, :, 3*Nt:4*Nt]\n",
    "\n",
    "    # refine bounding boxes\n",
    "    dcx = cw * tx[fy, fx, fc]\n",
    "    dcy = ch * ty[fy, fx, fc]\n",
    "    rcx = cx + dcx\n",
    "    rcy = cy + dcy\n",
    "    rcw = cw * np.exp(tw[fy, fx, fc])\n",
    "    rch = ch * np.exp(th[fy, fx, fc])\n",
    "\n",
    "    scores = score_cls_tf[0, fy, fx, fc]\n",
    "    tmp_bboxes = np.vstack((rcx - rcw / 2, rcy - rch / 2, rcx + rcw / 2, rcy + rch / 2))\n",
    "    tmp_bboxes = np.vstack((tmp_bboxes / s, scores))\n",
    "    tmp_bboxes = tmp_bboxes.transpose()\n",
    "    return tmp_bboxes\n",
    "\n",
    "\n",
    "def overlay_bounding_boxes(raw_img, refined_bboxes, lw):\n",
    "    \"\"\"Overlay bounding boxes of face on images.\n",
    "    Args:\n",
    "      raw_img:\n",
    "        A target image.\n",
    "      refined_bboxes:\n",
    "        Bounding boxes of detected faces.\n",
    "      lw: \n",
    "        Line width of bounding boxes. If zero specified,\n",
    "        this is determined based on confidence of each detection.\n",
    "    Returns:\n",
    "      None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Overlay bounding boxes on an image with the color based on the confidence.\n",
    "    for r in refined_bboxes:\n",
    "        _score = expit(r[4])\n",
    "        cm_idx = int(np.ceil(_score * 255))\n",
    "        rect_color = [int(np.ceil(x * 255)) for x in util.cm_data[cm_idx]]  # parula\n",
    "        _lw = lw\n",
    "        if lw == 0:  # line width of each bounding box is adaptively determined.\n",
    "            bw, bh = r[2] - r[0] + 1, r[3] - r[0] + 1\n",
    "            _lw = 1 if min(bw, bh) <= 20 else max(2, min(3, min(bh / 20, bw / 20)))\n",
    "            _lw = int(np.ceil(_lw * _score))\n",
    "\n",
    "        _r = [int(x) for x in r[:4]]\n",
    "        cv2.rectangle(raw_img, (_r[0], _r[1]), (_r[2], _r[3]), rect_color, _lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whirldata/tensorflow-dev/lib/python3.5/site-packages/ipykernel_launcher.py:6: MatplotlibDeprecationWarning: numpy.arange\n",
      "  \n",
      "/home/whirldata/tensorflow-dev/lib/python3.5/site-packages/ipykernel_launcher.py:7: MatplotlibDeprecationWarning: numpy.arange\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 19.69 secs for single_face.jpg\n",
      "Number of Faces ==> 1\n",
      "time 80.45 secs for multiple_faces.jpg\n",
      "Number of Faces ==> 12\n",
      "time 78.29 secs for group_faces.jpg\n",
      "Number of Faces ==> 41\n"
     ]
    }
   ],
   "source": [
    "weight_file_path = \"model.pkl\"\n",
    "prob_thresh = 0.5\n",
    "nms_thresh = 0.1\n",
    "lw = 3\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(tf.float32, [1, None, None, 3]) # n, h, w, c\n",
    "\n",
    "    # Create the tiny face model which weights are loaded from a pretrained model.\n",
    "    model = tiny_face_model.Model(weight_file_path)\n",
    "    score_final = model.tiny_face(x)\n",
    "\n",
    "    # Find image files in data_dir.\n",
    "    filenames = []\n",
    "    for ext in ('*.png', '*.gif', '*.jpg', '*.jpeg',\"*.JPG\"):\n",
    "        filenames.extend(glob.glob(os.path.join(\"images/\", ext)))\n",
    "#     print(filenames)\n",
    "\n",
    "    # Load an average image and clusters(reference boxes of templates).\n",
    "    with open(weight_file_path, \"rb\") as f:\n",
    "        _, mat_params_dict = pickle.load(f)\n",
    "\n",
    "    average_image = model.get_data_by_key(\"average_image\")\n",
    "    clusters = model.get_data_by_key(\"clusters\")\n",
    "    clusters_h = clusters[:, 3] - clusters[:, 1] + 1\n",
    "    clusters_w = clusters[:, 2] - clusters[:, 0] + 1\n",
    "    normal_idx = np.where(clusters[:, 4] == 1)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for filename in filenames:\n",
    "            fname = filename.split(os.sep)[-1]\n",
    "            raw_img = cv2.imread(filename)\n",
    "            raw_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)\n",
    "            raw_img_f = raw_img.astype(np.float32)\n",
    "\n",
    "      \n",
    "\n",
    "            scales = _calc_scales()\n",
    "            start = time.time()\n",
    "\n",
    "            # initialize output\n",
    "            bboxes = np.empty(shape=(0, 5))\n",
    "\n",
    "            # process input at different scales\n",
    "            for s in scales:\n",
    "#                 print(\"Processing {} at scale {:.4f}\".format(fname, s))\n",
    "                img = cv2.resize(raw_img_f, (0, 0), fx=s, fy=s, interpolation=cv2.INTER_LINEAR)\n",
    "                img = img - average_image\n",
    "                img = img[np.newaxis, :]\n",
    "\n",
    "                # we don't run every template on every scale ids of templates to ignore\n",
    "                tids = list(range(4, 12)) + ([] if s <= 1.0 else list(range(18, 25)))\n",
    "                ignoredTids = list(set(range(0, clusters.shape[0])) - set(tids))\n",
    "\n",
    "                # run through the net\n",
    "                score_final_tf = sess.run(score_final, feed_dict={x: img})\n",
    "\n",
    "                # collect scores\n",
    "                score_cls_tf, score_reg_tf = score_final_tf[:, :, :, :25], score_final_tf[:, :, :, 25:125]\n",
    "                prob_cls_tf = expit(score_cls_tf)\n",
    "                prob_cls_tf[0, :, :, ignoredTids] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "                tmp_bboxes = _calc_bounding_boxes()\n",
    "                bboxes = np.vstack((bboxes, tmp_bboxes)) # <class 'tuple'>: (5265, 5)\n",
    "\n",
    "\n",
    "            print(\"time {:.2f} secs for {}\".format(time.time() - start, fname))\n",
    "\n",
    "            # non maximum suppression\n",
    "            # refind_idx = util.nms(bboxes, nms_thresh)\n",
    "            refind_idx = tf.image.non_max_suppression(tf.convert_to_tensor(bboxes[:, :4], dtype=tf.float32),\n",
    "                                                       tf.convert_to_tensor(bboxes[:, 4], dtype=tf.float32),\n",
    "                                                       max_output_size=bboxes.shape[0], iou_threshold=nms_thresh)\n",
    "            refind_idx = sess.run(refind_idx)\n",
    "            refined_bboxes = bboxes[refind_idx]\n",
    "            print(\"Number of Faces ==>\",len(refined_bboxes))\n",
    "\n",
    "            overlay_bounding_boxes(raw_img, refined_bboxes, lw)\n",
    "            raw_img = cv2.cvtColor(raw_img, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(\"output_images/tiny_tensor_{}.jpg\".format(filename.split(\"/\")[1]), raw_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Faces ==> 1\n",
      "CPU times: user 329 ms, sys: 3.91 ms, total: 333 ms\n",
      "Wall time: 331 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "img = cv2.imread(\"images/single_face.jpg\")\n",
    "face_locations = face_recognition.face_locations(img)\n",
    "\n",
    "print(\"Number of Faces ==>\",len(face_locations))\n",
    "\n",
    "for top, right, bottom, left in face_locations:\n",
    "        cv2.rectangle(img, (left, top), (right, bottom), (255, 0, 255), 2)\n",
    "cv2.imwrite(\"output_images/face_recognition_single_face.jpg\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Faces ==> 11\n",
      "CPU times: user 1.24 s, sys: 3.27 ms, total: 1.24 s\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "img = cv2.imread(\"images/multiple_faces.jpg\")\n",
    "face_locations = face_recognition.face_locations(img)\n",
    "\n",
    "print(\"Number of Faces ==>\",len(face_locations))\n",
    "\n",
    "for top, right, bottom, left in face_locations:\n",
    "        cv2.rectangle(img, (left, top), (right, bottom), (255, 0, 255), 2)\n",
    "cv2.imwrite(\"output_images/face_recognition_multiple_face.jpg\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Faces ==> 39\n",
      "CPU times: user 7.18 s, sys: 209 ms, total: 7.39 s\n",
      "Wall time: 7.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "img = cv2.imread(\"images/group_faces.jpg\")\n",
    "face_locations = face_recognition.face_locations(img)\n",
    "\n",
    "print(\"Number of Faces ==>\",len(face_locations))\n",
    "\n",
    "for top, right, bottom, left in face_locations:\n",
    "        cv2.rectangle(img, (left, top), (right, bottom), (255, 0, 255), 5)\n",
    "cv2.imwrite(\"output_images/face_recognition_group_face.jpg\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
